# -*- coding: utf-8 -*-
"""Bible Probas GPT2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1suo_qf2DdbU0j1biV-Z2iKCKF5qtQgSt

# Load libraries and model
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers

from google.colab import drive
drive.mount('/content/gdrive')

gdrive_path = '/content/gdrive/My Drive/'

import sys
sys.path.append(gdrive_path)

from transformers import pipeline
import numpy as np
import torch
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer
import data

device = "cpu"
gpt2 = AutoModelForCausalLM.from_pretrained("gpt2", return_dict_in_generate=True).to(device)
tokenizer = AutoTokenizer.from_pretrained("gpt2")

"""# Load the data"""

bible_filename = "eng-x-bible-world.txt"
bible = data.parse_pbc_bible(gdrive_path + bible_filename)

# Add SOS and EOS tokens
delimited_3 = ['\n\n\n' + seq + tokenizer.eos_token for seq in bible.content.values()]
delimited_3_ids = [tokenizer(seq, return_tensors="pt").input_ids.to(device) for seq in delimited_3]

# Do the same, but start with a double newline
delimited_2 = ['\n\n' + seq + tokenizer.eos_token for seq in bible.content.values()]
delimited_2_ids = [tokenizer(seq, return_tensors="pt").input_ids.to(device) for seq in delimited_2]

# And now only one
delimited_1 = ['\n' + seq + tokenizer.eos_token for seq in bible.content.values()]
delimited_1_ids = [tokenizer(seq, return_tensors="pt").input_ids.to(device) for seq in delimited_1]

# Deleted abnormally short sequences
assert all([len([seq for seq in seqs if len(seq[0]) < 3]) < 5 for seqs in (delimited_1_ids, delimited_2_ids, delimited_3_ids)])
delimited_1_ids = [seq for seq in delimited_1_ids if len(seq[0]) >= 3]
delimited_2_ids = [seq for seq in delimited_2_ids if len(seq[0]) >= 3]
delimited_3_ids = [seq for seq in delimited_3_ids if len(seq[0]) >= 3]

# Assert that we understand what the prompt tokens look like
assert all([seq[0][0] == 628 and seq[0][1] == 198 for seq in delimited_3_ids])
assert all([seq[0][0] == 198 and seq[0][1] == 198 for seq in delimited_2_ids])
assert all([seq[0][0] == 198 for seq in delimited_1_ids])

# Assert that everything ends with the EOS token
assert all([all([seq[0][-1] == tokenizer.eos_token_id for seq in seqs]) for seqs in (delimited_1_ids, delimited_2_ids, delimited_3_ids)])

"""Now feed these into GPT-2 and compute the probabilities. We are nearly done with this. The last step is to compute the Shannon entropy, and then to compute the entropy for randomized sequences (maybe in that case we should change the capitalization too, as a check).

**WARNING**: Eduardo says that GPT-2 is not good for short sequences

# Feed into GPT2
"""

loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')
prompt_tokens = (1, 2, 2)

seqs = delimited_2_ids
batch = seqs[0]
seq = batch[0]
outputs = gpt2(seq)
n_prompt_tokens = prompt_tokens[1]
n_input_tokens = len(seq)
fn_loss = loss_fn(outputs.logits[n_prompt_tokens-1:n_input_tokens-1], seq[n_prompt_tokens:])

gpt2.eval()
prompt_tokens = (1, 2, 2)
loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')
with torch.no_grad():
  newlines_losses = {}
  for i, seqs in enumerate((delimited_1_ids, delimited_2_ids, delimited_3_ids)):
    sequence_losses = []
    for batch in seqs:
      assert len(batch) == 1
      seq = batch[0]
      outputs = gpt2(seq)
      n_prompt_tokens = prompt_tokens[i]
      n_input_tokens = len(seq)
      fn_loss = loss_fn(outputs.logits[n_prompt_tokens-1:n_input_tokens-1], seq[n_prompt_tokens:])
      sequence_losses.append(fn_loss)
    newlines_losses[i+1] = sequence_losses

newlines_losses_float = {k:[el.item() for el in v] for k,v in newlines_losses.items()}

"""Each of the losses is the negative log likelihood of a sequence. To estimate the entropy, we sum over these negative log likelihoods and divide by the number of sequences."""

newlines_entropy = {k:sum(v)/len(v) for k,v in newlines_losses_float.items()}

newlines_entropy

"""This value of the entropy in itself does not tell us much. It is reassuring that it is very similar among the three newline values that we tried. Next, we can do the following (in no particular order):

1. compare with other English bibles to see if we get similar values
2. compare with using a full stop as an end-of-sentence token
3. think of how to validate this model to know that the computation makes sense
4. continue with the calculation of the entropy as required for my study
"""

with open(gdrive_path + 'losses_' + bible_filename, 'w') as f:
  for k,v in newlines_losses_float.items():
    f.write(str(k) + '\n')
    f.write('--------------------' + '\n')
    f.write(','.join([str(el) for el in v]) + '\n')
    f.write('--------------------' + '\n')
  f.write(','.join([f'{k}: {v}' for k,v in newlines_entropy.items()]) + '\n')

