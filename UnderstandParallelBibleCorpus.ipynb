{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import data\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_BIBLE_CORPUS_PATH = '/home/pablo/Documents/GitHubRepos/paralleltext/bibles/corpus'\n",
    "KOPLENIG_FILES = '/home/pablo/ownCloud/WordOrderBibles/Literature/ThirdRound/dataverse_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant, bible, shuffled, master = [pd.read_csv(os.path.join(KOPLENIG_FILES, filename), sep='\\t') for filename in (\n",
    "    'final_data_entropy_bible_constant.csv', \n",
    "    'final_data_entropy_bible.csv', \n",
    "    'final_data_entropy_bible_constant_fullshuffle.csv', \n",
    "    'master.csv'\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant[(constant['language'].apply(lambda x: 'Auhelawa' in x)) & (constant['booktitle'] == 'Luke')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled[(shuffled['language'].apply(lambda x: 'Auhelawa' in x)) & (shuffled['booktitle'] == 'Luke')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master['language'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl, grp in master.groupby('language'):\n",
    "    if len(grp) > 1:\n",
    "        print(lbl)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master[master['language'] == 'Afrikaans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.groupby('ISO').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the names of the languages for which there are multiple ISOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl, grp in master.groupby('language'):\n",
    "    if grp['ISO'].nunique() > 1:\n",
    "        print(lbl, grp['ISO'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, get the ISOs for which there are multiple languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lbl, grp in master.groupby('ISO'):\n",
    "    if grp['language'].nunique() > 1:\n",
    "        print(lbl, grp['language'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would probably be better to trust the ISO code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffled contains books, while master contains bibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert master['translation'].nunique() == len(master), 'There are multiple bibles with the same translation code'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not any([el for el in shuffled['translation'].tolist() if el not in master['translation'].tolist()]), \\\n",
    "'Some translation codes in the list of books are not in the list of bibles'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple compression method\n",
    "\n",
    "Let's test the simple compression method in Koplenig et al that is given as an example. I have downloaded a random Wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('randomWikipediaPage.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into words. This loses information, but we could keep this information in some way\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_n_alpha = sum([len(wd) for wd in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_n_spaces = len(text) - orig_n_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_words = {}\n",
    "new_list = []\n",
    "for wd in words:\n",
    "    if wd in seen_words:\n",
    "        new_list.append(str(seen_words[wd]))\n",
    "    else:\n",
    "        new_list.append(f'{len(seen_words)}_{wd}')\n",
    "        seen_words[wd] = len(seen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_n_alpha = sum([len(wd) for wd in new_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_size = orig_n_alpha + orig_n_spaces\n",
    "new_size = new_n_alpha + orig_n_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_size/old_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this didn't work for us. Probably it would be good to remove punctuation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [text[0]]\n",
    "old_ch = text[0]\n",
    "for ch in text[1:]:\n",
    "    if ch.isalpha() != old_ch.isalpha():\n",
    "        tokens.append(ch)\n",
    "    else:\n",
    "        tokens[-1] = tokens[-1] + ch\n",
    "    old_ch = ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This split the tokens differently, and now we can attempt the same process again. Note that now the number of characters in these tokens is the same as in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(text) == sum([len(wd) for wd in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_words = {}\n",
    "new_list = []\n",
    "for wd in tokens:\n",
    "    if wd in seen_words:\n",
    "        new_list.append(str(seen_words[wd]))\n",
    "    else:\n",
    "        new_list.append(f'{len(seen_words)}_{wd}')\n",
    "        seen_words[wd] = len(seen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))\n",
    "print(sum([len(wd) for wd in new_list]))\n",
    "print(sum([len(wd) for wd in new_list])/len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this gave some compression, but still very little. A final approach would be to represent non-alpha characters without modification. But we have to add an underscore to ranks to distinguish from numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_words = {}\n",
    "new_list = []\n",
    "for wd in tokens:\n",
    "    if not wd.isalpha():\n",
    "        new_list.append(wd)\n",
    "        continue\n",
    "    if wd in seen_words:\n",
    "        new_list.append(str(seen_words[wd]) + '_')\n",
    "    else:\n",
    "        new_list.append(f'{len(seen_words)}_{wd}')\n",
    "        seen_words[wd] = len(seen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))\n",
    "print(sum([len(wd) for wd in new_list]))\n",
    "print(sum([len(wd) for wd in new_list])/len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ended up hurting us. My conclusion would be that you need a bigger corpus in order for this compression algorithm to do something meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bible corpus\n",
    "\n",
    "The corpus was made available my Michael Cysouw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibles = os.listdir(OLD_BIBLE_CORPUS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([len(el.split('-')) for el in bibles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[el for el in bibles if len(el.split('-')) == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not [el for el in bibles if el.split('.')[1] != 'txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_bibles = []\n",
    "for bible_name in bibles:\n",
    "    bible = bible_name.split('.')[0]\n",
    "    parts = bible.split('-')\n",
    "    language = parts[0]\n",
    "    delimiter = parts[1]\n",
    "    document_type = parts[2]\n",
    "    description = '' if len(parts) == 3 else '-'.join(parts[3:])\n",
    "    uid = bible\n",
    "    filename = bible_name\n",
    "    struct_bibles.append((language, delimiter, document_type, description, uid, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not [el for el in struct_bibles if el[2] != 'bible']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_df = pd.DataFrame(columns=['language', 'delimiter', 'type', 'description', 'uid', 'filename'], \n",
    "                        data=struct_bibles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_df.loc[(bible_df['language'] == 'ben') & (bible_df['type'] == 'kerry'), 'description'] = 'kerry'\n",
    "bible_df.loc[(bible_df['language'] == 'ben') & (bible_df['type'] == 'kerry'), 'type'] = 'bible'\n",
    "bible_df.loc[(bible_df['language'] == 'ben') & (bible_df['type'] == 'kerry'), 'uid'] = 'ben-x-bible-kerry'\n",
    "assert len(struct_bibles) == len(bible_df)\n",
    "assert len(struct_bibles) == len(bible_df[(bible_df['delimiter'] == 'x') & (bible_df['type'] == 'bible')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have filled a table with the names of the files. We also need to load the contents of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bible(bible_lines: list, parse_content: bool) -> tuple:\n",
    "    # Assume that the file starts with comments, and then it moves on to content\n",
    "    # The comments should have string keys (not numeric) that start with a hash and whose key ends in colon\n",
    "    # The content can optionally be commented out\n",
    "    in_comments = True\n",
    "    comment_lines, content_lines = [], []\n",
    "    content_pattern = '#? ?(\\d{1,8}) ?\\t(.*)\\s*'\n",
    "    for line in bible_lines:\n",
    "        #print(line, in_comments)\n",
    "        if in_comments:\n",
    "            comment_match = re.fullmatch('# ([\\w\\d-]+):\\s+(.*)\\s*', line)\n",
    "            if comment_match:\n",
    "                comment_lines.append((comment_match.group(1), comment_match.group(2)))\n",
    "            else:\n",
    "                content_match = re.fullmatch(content_pattern, line)\n",
    "                if content_match:\n",
    "                    if not parse_content:\n",
    "                        break\n",
    "                    content_lines.append((content_match.group(1), content_match.group(2), line[0] == '#'))\n",
    "                    in_comments = False\n",
    "                else:\n",
    "                    comment_lines[-1] = (comment_lines[-1][0], comment_lines[-1][1] + '\\n' + line)\n",
    "        else:\n",
    "            content_match = re.fullmatch(content_pattern, line)\n",
    "            if content_match:\n",
    "                content_lines.append((content_match.group(1), content_match.group(2), line[0] == '#'))\n",
    "            else:\n",
    "                raise Exception(f'{line} does not match an expected format')\n",
    "    return comment_lines, content_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_parse(filename: str, parse_content: bool) -> tuple:\n",
    "    with open(os.path.join(OLD_BIBLE_CORPUS_PATH, filename)) as f:\n",
    "        lines = f.readlines()\n",
    "    return parse_bible(lines, parse_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_and_content = []\n",
    "for filename in bible_df['filename']:\n",
    "    if len(comments_and_content) % int(len(bible_df) / 10) == 0:\n",
    "        print(len(comments_and_content))\n",
    "    try:\n",
    "        comments_and_content.append(open_and_parse(filename, True))\n",
    "    except Exception as e:\n",
    "        print(f'Error for file {filename}', e)\n",
    "        comments_and_content.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st index: bible\n",
    "# 2nd index: comments or content\n",
    "# 3rd index: line number\n",
    "# 4th index: key, value, or commented (the latter in the case of content only)\n",
    "zeroth_bible_content_zeroth_line_is_commented = comments_and_content[0][1][0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some simple analytics\n",
    "\n",
    "### Are the keys in the comments universal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_in_comments = [[comment[0] for comment in bible[0]] for bible in comments_and_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comment_keys = [el for lis in keys_in_comments for el in lis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_key_counter = Counter(all_comment_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_key_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that all the keys are basically the same, with the exception of the notes, which for some reason are missing in two of the bibles. We can insert all of this into a dataframe. Maybe it will be the same as the dataframes we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dicts = []\n",
    "assert len(bible_df) == len(comments_and_content)\n",
    "for i, filename in enumerate(bible_df['filename']):\n",
    "    key_value = {k:v for k,v in comments_and_content[i][0]}\n",
    "    if 'notes' not in key_value:\n",
    "        key_value['notes'] = ''\n",
    "    key_value['filename'] = filename\n",
    "    comments_dicts.append(key_value)\n",
    "comments_df = pd.DataFrame(comments_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these two dataframes don't match exactly, although they're probably related.\n",
    "\n",
    "### How common is it for content lines to be commented out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bibles_with_commented_content = [i for i, comments_content in enumerate(comments_and_content) if any([el[2] for el in comments_content[1]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(bibles_with_commented_content)} bibles have commented-out content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_of_commented_content_lines = [len([el for el in comments_and_content[i][1] if el[2]]) for i in bibles_with_commented_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The top most commented bibles are: {sorted(numbers_of_commented_content_lines, key=lambda x: -x)[:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_num_commented_content_lines = [(ix, numbers_of_commented_content_lines[i]) for i, ix in enumerate(bibles_with_commented_content)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_num_commented_content_lines = sorted(index_num_commented_content_lines, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The most commented bible is {bible_df[\"filename\"].tolist()[index_num_commented_content_lines[0][0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df[comments_df['filename'] == bible_df[\"filename\"].tolist()[index_num_commented_content_lines[0][0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The total number of content lines in that file is {len(comments_and_content[index_num_commented_content_lines[0][0]][1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So about 10% of the content lines in that file are commented out. Now let's see what some of those lines look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_similar_lines = [el for el in comments_and_content[index_num_commented_content_lines[0][0]][1] if el[0] == '40026071']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(two_similar_lines[1][1], len(two_similar_lines[1][1]))\n",
    "print(two_similar_lines[0][1], len(two_similar_lines[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a very small difference in the word \"jah sa\" versus \"jas~sa\". This must be due to some difference in convention of writing. That could explain some of these commented-out lines. The best we can do is to ignore commented-out lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers of content lines with no key or no value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_key = [i for i, comments_content in enumerate(comments_and_content) if any([el[0].strip() == '' for el in comments_content[1]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_value = [i for i, comments_content in enumerate(comments_and_content) if any([el[1].strip() == '' for el in comments_content[1]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(no_key)} bibles have content lines with no key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(no_value)} bibles have content lines with no value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So having now value is quite common. Let's take one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welsh = 12\n",
    "assert welsh in no_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_empty_line = [(i, el) for i, el in enumerate(comments_and_content[welsh][1]) if not el[1].strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_and_content[welsh][1][first_empty_line[0][0]-1:first_empty_line[0][0]+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's translate the verse before the empty line:\n",
    "\n",
    "*Greet one another with a holy kiss. All the saints salute you.*\n",
    "\n",
    "Let's look for the same line in an English bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = 20\n",
    "empty_verse_index = [i for i in range(len(comments_and_content[english][1])) if comments_and_content[english][1][i][0] == '47013013'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_and_content[english][1][empty_verse_index-1:empty_verse_index+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, indeed, for some reason the Welsh bible has two verses merged into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of content lines for different bibles/languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commented_content_lines, uncommented_content_lines = [], []\n",
    "for bible_index, comment_content in enumerate(comments_and_content):\n",
    "    content = comment_content[1]\n",
    "    non_commented_content = [el for el in content if not el[2]]\n",
    "    commented_content = [el for el in content if el[2]]\n",
    "    assert len(commented_content) + len(non_commented_content) == len(content)\n",
    "    commented_content_lines.append(len(commented_content))\n",
    "    uncommented_content_lines.append(len(non_commented_content))\n",
    "    assert len(non_commented_content) == len(set([el[0] for el in non_commented_content]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(comments_df) == len(commented_content_lines) and len(comments_df) == len(uncommented_content_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['n_uncommented_verses'] = uncommented_content_lines\n",
    "comments_df['n_commented_verses'] = commented_content_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comments_df['n_uncommented_verses'], bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two peaks look like only new testament versus full bible. Let's take some example and check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comments_df[comments_df['n_uncommented_verses'] < 15000]['n_uncommented_verses'], bins=30)\n",
    "plt.title('Small counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comments_df[comments_df['n_uncommented_verses'] > 15000]['n_uncommented_verses'], bins=30)\n",
    "plt.title('Big counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verse_counter = Counter(comments_df['n_uncommented_verses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verse_counter.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_peak = verse_counter.most_common(1)[0][0]\n",
    "in_peak = [i for i in range(len(comments_df)) if comments_df['n_uncommented_verses'].tolist()[i] == n_peak][:3]\n",
    "print(f'Exactly the peak: {in_peak}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_second = verse_counter.most_common(2)[1][0]\n",
    "in_second = [i for i in range(len(comments_df)) if comments_df['n_uncommented_verses'].tolist()[i] == n_second][:3]\n",
    "print(f'Exactly the secondary peak: {in_second}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_almost = verse_counter.most_common(3)[2][0]\n",
    "in_almost = [i for i in range(len(comments_df)) if comments_df['n_uncommented_verses'].tolist()[i] == n_almost][:3]\n",
    "print(f'Almost the peak: {in_almost}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.iloc[in_peak]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.iloc[in_second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.iloc[in_almost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the large peak at smaller number of verses is the new testament only, while the secondary peak at larger number of verses is the full bible. Why are the counts not exactly the same for all bibles in each peak?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_new_testament = comments_and_content[in_peak[0]][1]\n",
    "almost_new_testament = comments_and_content[in_almost[0]][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_new_testament_keys = [el[0] for el in full_new_testament if not el[2]]\n",
    "almost_new_testament_keys = [el[0] for el in almost_new_testament if not el[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The full and almost full new testaments contain {len(full_new_testament_keys)} and {len(almost_new_testament_keys)} keys, respectively')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys in (full_new_testament_keys, almost_new_testament_keys):\n",
    "    assert len(keys) == len(set(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len([el for el in almost_new_testament_keys if el not in full_new_testament_keys]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len([el for el in full_new_testament_keys if el not in almost_new_testament_keys]) == 1\n",
    "missing_key = [el for el in full_new_testament_keys if el not in almost_new_testament_keys][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[el for el in almost_new_testament if el[0] == missing_key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's fully missing, not commented out. Let's look at the raw file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OLD_BIBLE_CORPUS_PATH, comments_df.iloc[in_peak[0]]['filename']), 'r') as f:\n",
    "    peak_lines = f.readlines()\n",
    "with open(os.path.join(OLD_BIBLE_CORPUS_PATH, comments_df.iloc[in_almost[0]]['filename']), 'r') as f:\n",
    "    almost_lines = f.readlines()\n",
    "for i, lines in enumerate((peak_lines, almost_lines)):\n",
    "    print(i)\n",
    "    print([line for line in lines if missing_key in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line is not in the second bible, even commented out or mis-parsed. It's not there at all. I'd like to see this in an English bible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_filename = comments_df[(comments_df['language_name'].apply(lambda x: 'English' == x.strip())) & (comments_df['n_uncommented_verses'] == n_peak)]['filename'].tolist()[0]\n",
    "with open(os.path.join(OLD_BIBLE_CORPUS_PATH, english_filename)) as f:\n",
    "    english_lines = f.readlines()\n",
    "print(english_filename)\n",
    "[line for line in english_lines if missing_key in line]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not seem to be anything special about this verse. I need some information about it. 43 is the book of John. John 7:53 is a verse that is apparently joined with the following verse in some bibles. Specifically, with the first verse in chapter 8. So let's look at this verse in the bible that is missing the verse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df[comments_df['n_uncommented_verses'] == n_almost]['language_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malay is widely spoken and easy to translate, so let's pick that one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "almost_index = comments_df[comments_df.apply(lambda row: row['n_uncommented_verses'] == n_almost and row['language_name'].strip() == 'Malay', 1)].index.tolist()[0]\n",
    "[el for el in comments_and_content[almost_index][1] if '43008001' == el[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Google Translate, this translates to \"Then everyone went home, but Jesus went to the Mount of Olives.\". Thus, indeed, this New Testament merges 7:53 with 8:1. What does 8:1 look like in the English bible referenced above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[el for el in english_lines if '43008001' in el]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, this makes no reference to everyone going home. So the verses are separated. We will have to deal with this if we want to do verse matching. However, it's not clear that that is what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data characteristics in Bentz et al\n",
    "\n",
    "Do my simple analytics match what is reported in Bentz et al?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_content = {file: comments_and_content[i][1] for i, file in enumerate(comments_df['filename'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only texts with at least\n",
    "50 K tokens are included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bible_text(bible: list) -> str:\n",
    "    text = ''\n",
    "    for verse in bible:\n",
    "        if verse[2]:\n",
    "            continue\n",
    "        text += (verse[1] + '\\n')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [merge_bible_text(el[1]) for el in comments_and_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens_in_bible(bible: str) -> int:\n",
    "    return len(re.findall('\\s\\S', bible.strip())) + 1\n",
    "\n",
    "comments_df['n_tokens'] = [count_tokens_in_bible(el) for el in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df['n_tokens'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_cutoff = 50000\n",
    "    \n",
    "file_text = {file: texts[i] for i, file in enumerate(comments_df['filename'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_enough = comments_df[comments_df['n_tokens'] > token_cutoff].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{len(long_enough)} bibles pass the token cutoff')\n",
    "print(f'This represents {int(len(long_enough) / len(comments_df) * 100)}% of the bibles')\n",
    "print(f'For Bentz et al, this was {int(1499/1525*100)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the number of bibles I have is much bigger than the one used by Bentz et al, and this is probably due to changes in the corpus since then. But both of us have nearly all bibles long enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of languages goes from {comments_df['closest_ISO_639-3'].nunique()} to {long_enough['closest_ISO_639-3'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very small reduction, as was also observed by Bentz et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n_languages/n_bibles={long_enough['closest_ISO_639-3'].nunique()/len(long_enough)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'For Bentz et al, it was {1115/1499}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is very similar to previous work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The mean size of a bible (including short ones) is {comments_df[\"n_tokens\"].mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is considerably longer (though on the same order of magnitude) as reported by Bentz et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The total number of tokens is {int(comments_df[\"n_tokens\"].sum()/1000000)}M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for the comparison with Bentz et al. The longer mean size might be due to improvements to the corpus, or due to a difference in token counting. Since it's on the same order of magnitude, I will not worry about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list:\n",
    "    return re.sub(r'[^\\w\\s]','',text).lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenize(el[1]) for el in comments_and_content[0][1]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_and_content[0][1][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many bibles contain the underscore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "underscored_files = []\n",
    "for i, filename in enumerate(bible_df['filename'].tolist()):\n",
    "    with open(os.path.join(OLD_BIBLE_CORPUS_PATH, filename), 'r') as f:\n",
    "        text = f.readlines()\n",
    "        if any(['_' in line and line[0] != '#' for line in text]):\n",
    "            print(filename)\n",
    "            underscored_files.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[el[1] for el in comments_and_content[i][0] if el[0] == 'language_name'] for i in underscored_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked at the two that are most familiar to me (Polish and Filipino), and found in the case of Filipino that the underscores could just be removed. In the case of Polish it is not so clear. Seeing as how these are very few bibles, I would just remove them from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenization, which bibles (if any) contain a character that is not a letter, a number, an underscore or a whitespace?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, bible in enumerate(comments_and_content):\n",
    "    bible = bible[1]\n",
    "    bible = '\\n'.join([el[1] for el in bible])\n",
    "    tokens = tokenize(bible)\n",
    "    bible = ' '.join(tokens)\n",
    "    if not re.fullmatch('[\\w\\s]*', bible):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this seems to be related to few examples..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tokenization\n",
    "\n",
    "We follow the methodology of Bentz et al for tokenization: the PBC has spaces between punctuation marks, so we split on spaces, and then we remove punctuation by keeping only tokens that contain at least one non-punctuation character. Thus, the word \"she's\" should appear in the corpus as \"she ' s\", and be tokenized as \"she s\". Meanwhile, the word \"q'atb'altzij\" should appear in the corpus as \"q'atb'altzij\", and be tokenized as \"q'atb'altzij\". We will test those cases here, as well as 3 random verses from 3 random bibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [\"she\", \"s\"] == data.tokenize(\"she ' s\", remove_punctuation=True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [\"q'atb'altzij\"] == data.tokenize(\"q'atb'altzij\", remove_punctuation=True, lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bibles = [os.path.join(OLD_BIBLE_CORPUS_PATH, filename) \\\n",
    "                 for filename in bible_df['filename'].sample(3).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokenized_bibles = [data.process_bible(filename, 'PBC') \\\n",
    "                           for filename in sample_bibles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_original_bibles = [data.parse_file(filename, 'PBC') \\\n",
    "                          for filename in sample_bibles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([len(sample_original_bibles[i].content) == len(sample_tokenized_bibles[i].verse_tokens) \\\n",
    "            for i in range(len(sample_bibles))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sample_bibles)):\n",
    "    print(sample_bibles[i])\n",
    "    print(ix)\n",
    "    verse_number = random.choice(list(sample_original_bibles[i].content.keys()))\n",
    "    print(verse_number)\n",
    "    print(sample_original_bibles[i].content[verse_number])\n",
    "    print(' '.join(sample_tokenized_bibles[i].verse_tokens[verse_number]))\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all look good, so the tokenizer works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting\n",
    "\n",
    "Following Hahn, Degen & Furtrell (2021), we want to:\n",
    "\n",
    "* conserve 15% of the data as a held-out set for early stopping, learning curves, and hyperparameter estimation\n",
    "\n",
    "* concatenate the sentences from each partition in random order, separated by an end-of-sentence symbol\n",
    "\n",
    "Additionally, we are trying to estimate the entropy on the same data that we use to train the LSTM. This is a bit tricky, but on the other hand presumably we will not be minimizing the same metric during training, so it's probably OK. Still, it would be good to keep a test set for reporting. So we'll do a 75, 15, 10 split. What is a suitable end-of-sentence symbol? I think `<END>` would do, and we know it will not appear in any bible because they are lowercased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bibles = [os.path.join(OLD_BIBLE_CORPUS_PATH, filename) \\\n",
    "                 for filename in bible_df['filename'].sample(30).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokenized_bibles = [data.process_bible(filename, 'PBC') \\\n",
    "                           for filename in sample_bibles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_split_bibles = [bible.split(0.15, 0.1) for bible in sample_tokenized_bibles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No lines were lost\n",
    "assert all([len(bible.train_data) + len(bible.hold_out_data) + len(bible.test_data) == \\\n",
    "            len(sample_tokenized_bibles[i].verse_tokens) \\\n",
    "            for i, bible in enumerate(sample_split_bibles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No tokens were lost\n",
    "for i in range(len(sample_bibles)):\n",
    "    verse_tokens = sample_tokenized_bibles[i].verse_tokens\n",
    "    split_data = sample_split_bibles[i]\n",
    "    split_data = (split_data.train_data, split_data.hold_out_data, split_data.test_data)\n",
    "    n_orig_tokens = sum([len(v) for v in verse_tokens.values()])\n",
    "    n_split_tokens = sum([sum([len(verse) for verse in data_partition]) \\\n",
    "                          for data_partition in split_data])\n",
    "    assert n_orig_tokens == n_split_tokens, (n_orig_tokens, n_split_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No types were lost\n",
    "for i in range(len(sample_bibles)):\n",
    "    verse_tokens = sample_tokenized_bibles[i].verse_tokens\n",
    "    split_data = sample_split_bibles[i]\n",
    "    split_data = (split_data.train_data, split_data.hold_out_data, split_data.test_data)\n",
    "    n_orig_types = set([el for lis in verse_tokens.values() for el in lis])\n",
    "    n_split_types = set([ell for liss in [[el for lis in data_partition for el in lis] \\\n",
    "                                          for data_partition in split_data] for ell in liss])\n",
    "    assert n_orig_types == n_split_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
